"""
Mock RAG Agent for local testing without external dependencies
"""

import os
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Chunk(BaseModel):
    """Represents a content chunk retrieved from Qdrant"""
    content: str
    source_document: str
    page_number: Optional[int] = None
    section_title: Optional[str] = None
    similarity_score: float
    chunk_id: str


class QueryRequest(BaseModel):
    """Request model for querying the RAG agent"""
    query: str
    provider: str = "openai"  # "openai" or "gemini"
    max_chunks: int = 5
    user_id: Optional[str] = None


class RAGResponse(BaseModel):
    """Response model for RAG agent responses"""
    answer: str
    sources: List[Chunk]
    provider_used: str
    debug_info: Dict[str, Any]


class MockRAGAgent:
    """
    Mock RAG Agent for local testing without external dependencies
    """

    def __init__(self):
        logger.info("Initialized Mock RAG Agent for local testing")
        # Mock initialization - no external connections needed

    def retrieve_chunks(self, query: str, max_chunks: int = 5) -> List[Chunk]:
        """
        Mock implementation that returns sample chunks
        """
        start_time = time.time()
        logger.info(f"Mock retrieval for query: {query[:50]}...")

        # Create mock chunks with sample content
        mock_chunks = []
        for i in range(min(max_chunks, 3)):  # Return max 3 mock chunks
            chunk = Chunk(
                content=f"This is sample content related to '{query}'. This is mock content for testing the RAG interface. In a real implementation, this would be actual content retrieved from the vector database based on semantic similarity to the query.",
                source_document=f"sample_document_{i+1}.md",
                page_number=i + 1,
                section_title="Introduction" if i == 0 else f"Section {i+1}",
                similarity_score=0.8 - (i * 0.1),  # Decreasing similarity
                chunk_id=f"mock_chunk_{i}"
            )
            mock_chunks.append(chunk)

        retrieval_time = time.time() - start_time
        logger.info(f"Mock retrieved {len(mock_chunks)} chunks in {retrieval_time:.2f} seconds")
        return mock_chunks

    def generate_answer_with_mock(self, query: str, chunks: List[Chunk]) -> str:
        """
        Mock answer generation
        """
        start_time = time.time()
        logger.info(f"Generating mock answer for query: {query}")

        # Format context from chunks
        context = "\n\n".join([f"Source: {chunk.source_document}\nContent: {chunk.content}" for chunk in chunks])

        # Generate a mock answer based on the query
        mock_answers = {
            "humanoid": f"Based on the provided context, humanoid robotics is a fascinating field that combines mechanical engineering, artificial intelligence, and control systems. The key aspects include bipedal locomotion, human-like interaction, and advanced sensory systems. Query: '{query}'",
            "robotics": f"Robotics is an interdisciplinary field that integrates computer science, electrical engineering, and mechanical engineering. From the provided context: {context[:200]}...",
            "ai": f"Artificial Intelligence in robotics enables machines to perceive, reason, and act in complex environments. Based on the provided documents: {context[:200]}...",
        }

        # Use specific answer if query contains certain keywords, otherwise generic
        answer = mock_answers.get(query.lower()[:7], f"Based on the provided context: '{context[:300]}...', here's an answer to your question: '{query}'. This is a mock response for testing purposes. In a production environment, this would be generated by an AI model using the retrieved context.")

        generation_time = time.time() - start_time
        logger.info(f"Generated mock answer in {generation_time:.2f} seconds")
        return answer

    def query(self, query: str, provider: str = "openai", max_chunks: int = 5) -> RAGResponse:
        """
        Main method to query the mock RAG agent
        """
        start_time = time.time()
        logger.info(f"Processing mock query: {query}")

        # Step 1: Retrieve mock chunks
        chunks = self.retrieve_chunks(query, max_chunks)

        # Step 2: Generate mock answer
        answer = self.generate_answer_with_mock(query, chunks)
        provider_used = provider

        # Step 3: Prepare response with debug information
        total_time = time.time() - start_time
        debug_info = {
            "embedding_time": 0.01,  # Mock timing
            "retrieval_time": 0.05,
            "generation_time": 0.10,
            "total_time": total_time,
            "mock_response": True  # Flag to indicate this is a mock response
        }

        response = RAGResponse(
            answer=answer,
            sources=chunks,
            provider_used=provider_used,
            debug_info=debug_info
        )

        logger.info(f"Mock query completed in {total_time:.2f} seconds using {provider_used}")
        return response


# Example usage
if __name__ == "__main__":
    # Initialize the mock RAG agent
    agent = MockRAGAgent()

    # Example query
    query_request = QueryRequest(
        query="What are the key principles of humanoid robotics?",
        provider="openai",
        max_chunks=3
    )

    # Process the query
    response = agent.query(
        query=query_request.query,
        provider=query_request.provider,
        max_chunks=query_request.max_chunks
    )

    # Print the results
    print(f"Answer: {response.answer}")
    print(f"Sources: {len(response.sources)} chunks used")
    print(f"Provider: {response.provider_used}")
    print(f"Debug Info: {response.debug_info}")