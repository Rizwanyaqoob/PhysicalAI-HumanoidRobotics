"use strict";(globalThis.webpackChunkhumanoid_robotics_docs=globalThis.webpackChunkhumanoid_robotics_docs||[]).push([[488],{8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}},9165:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"perception/index","title":"Perception Systems","description":"Understanding computer vision, sensor fusion, and state estimation for humanoid robots","source":"@site/docs/perception/index.md","sourceDirName":"perception","slug":"/docs/perception","permalink":"/PhysicalAI-HumanoidRobotics/docs/docs/perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Rizwanyaqoob/Humanoid-Robotics-A-Practical-Introduction/tree/master/docs/perception/index.md","tags":[],"version":"current","frontMatter":{"title":"Perception Systems","description":"Understanding computer vision, sensor fusion, and state estimation for humanoid robots","slug":"/docs/perception"},"sidebar":"tutorialSidebar","previous":{"title":"Digital Twins for Humanoid Robotics","permalink":"/PhysicalAI-HumanoidRobotics/docs/docs/digital-twin"},"next":{"title":"Perception Systems","permalink":"/PhysicalAI-HumanoidRobotics/docs/docs/perception"}}');var s=i(4848),o=i(8453);const r={title:"Perception Systems",description:"Understanding computer vision, sensor fusion, and state estimation for humanoid robots",slug:"/docs/perception"},a="Perception Systems",l={},c=[{value:"Motivation",id:"motivation",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Computer Vision for Robotics",id:"computer-vision-for-robotics",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:3},{value:"State Estimation",id:"state-estimation",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Image Processing Pipeline",id:"image-processing-pipeline",level:3},{value:"Sensor Fusion with Kalman Filter",id:"sensor-fusion-with-kalman-filter",level:3},{value:"Code Blocks",id:"code-blocks",level:2},{value:"Multi-Camera Setup for 360\xb0 Perception",id:"multi-camera-setup-for-360-perception",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Quiz",id:"quiz",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"perception-systems",children:"Perception Systems"})}),"\n",(0,s.jsx)(n.h2,{id:"motivation",children:"Motivation"}),"\n",(0,s.jsx)(n.p,{children:"Perception systems are the eyes and ears of humanoid robots, enabling them to understand and interact with their environment. This module covers the essential concepts of computer vision, sensor fusion, and state estimation that allow humanoid robots to perceive the world around them."}),"\n",(0,s.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"computer-vision-for-robotics",children:"Computer Vision for Robotics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Image processing and feature detection"}),"\n",(0,s.jsx)(n.li,{children:"Object recognition and tracking"}),"\n",(0,s.jsx)(n.li,{children:"Depth estimation and 3D reconstruction"}),"\n",(0,s.jsx)(n.li,{children:"Visual SLAM for localization"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Combining data from multiple sensors (LiDAR, cameras, IMUs)"}),"\n",(0,s.jsx)(n.li,{children:"Kalman filtering and particle filters"}),"\n",(0,s.jsx)(n.li,{children:"Handling sensor noise and uncertainty"}),"\n",(0,s.jsx)(n.li,{children:"Multi-modal perception"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"state-estimation",children:"State Estimation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Robot pose estimation"}),"\n",(0,s.jsx)(n.li,{children:"Environment mapping"}),"\n",(0,s.jsx)(n.li,{children:"Dynamic object tracking"}),"\n",(0,s.jsx)(n.li,{children:"Uncertainty quantification"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,s.jsx)(n.h3,{id:"image-processing-pipeline",children:"Image Processing Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\n\ndef detect_objects(image):\n    """Detect objects in an image using computer vision techniques"""\n    # Convert to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Apply Gaussian blur to reduce noise\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n\n    # Detect edges using Canny edge detector\n    edges = cv2.Canny(blurred, 50, 150)\n\n    return edges\n\n# Example usage\nimage = cv2.imread(\'robot_view.jpg\')\nedges = detect_objects(image)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"sensor-fusion-with-kalman-filter",children:"Sensor Fusion with Kalman Filter"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\n\nclass KalmanFilter:\n    def __init__(self, process_noise, measurement_noise, initial_state, initial_uncertainty):\n        self.F = np.eye(len(initial_state))  # State transition model\n        self.H = np.eye(len(initial_state))  # Observation model\n        self.Q = np.eye(len(initial_state)) * process_noise  # Process noise\n        self.R = np.eye(len(initial_state)) * measurement_noise  # Measurement noise\n        self.P = np.eye(len(initial_state)) * initial_uncertainty  # Error covariance\n        self.x = np.array(initial_state)  # State estimate\n\n    def predict(self):\n        # Predict step\n        self.x = np.dot(self.F, self.x)\n        self.P = np.dot(np.dot(self.F, self.P), self.F.T) + self.Q\n\n    def update(self, measurement):\n        # Update step\n        y = measurement - np.dot(self.H, self.x)  # Innovation\n        S = np.dot(np.dot(self.H, self.P), self.H.T) + self.R  # Innovation covariance\n        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))  # Kalman gain\n\n        self.x = self.x + np.dot(K, y)\n        self.P = np.dot((np.eye(len(self.P)) - np.dot(K, self.H)), self.P)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"code-blocks",children:"Code Blocks"}),"\n",(0,s.jsx)(n.h3,{id:"multi-camera-setup-for-360-perception",children:"Multi-Camera Setup for 360\xb0 Perception"}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\n    A[Front Camera] --\x3e D[Fusion Node]\n    B[Side Camera] --\x3e D\n    C[Top Camera] --\x3e D\n    D --\x3e E[3D Reconstruction]\n    E --\x3e F[Object Detection]"}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.p,{children:"Common issues in perception systems:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Lighting conditions affecting computer vision"}),"\n",(0,s.jsx)(n.li,{children:"Sensor calibration drift"}),"\n",(0,s.jsx)(n.li,{children:"Data synchronization between multiple sensors"}),"\n",(0,s.jsx)(n.li,{children:"Computational constraints in real-time processing"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"quiz",children:"Quiz"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What are the three main components of a perception system?"}),"\n",(0,s.jsx)(n.li,{children:"How does sensor fusion improve robot perception?"}),"\n",(0,s.jsx)(n.li,{children:"What is the difference between visual SLAM and traditional SLAM?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.p,{children:["Continue to ",(0,s.jsx)(n.a,{href:"../motion-planning/",children:"Motion Planning"})," to learn about how humanoid robots plan their movements and navigate their environment."]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);