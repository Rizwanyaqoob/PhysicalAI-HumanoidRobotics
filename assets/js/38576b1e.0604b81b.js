"use strict";(globalThis.webpackChunkhumanoid_robotics_docs=globalThis.webpackChunkhumanoid_robotics_docs||[]).push([[84],{1885:i=>{i.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Introduction","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/docs/intro","label":"Introduction to Humanoid Robotics","docId":"intro/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/docs/intro"},{"type":"category","label":"Foundations","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/docs/foundations/embodied-intelligence","label":"Embodied Intelligence","docId":"foundations/embodied-intelligence","unlisted":false},{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/docs/foundations","label":"Robotics Foundations","docId":"foundations/index","unlisted":false},{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/docs/foundations/physical-ai","label":"Physical AI","docId":"foundations/physical-ai","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/docs/foundations"},{"type":"category","label":"ROS 2","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/docs/ros2/humanoid-control","label":"Humanoid Control","docId":"ros2/humanoid-control","unlisted":false},{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/docs/ros2","label":"ROS 2 - The Robotic Nervous System","docId":"ros2/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/docs/ros2"},{"type":"category","label":"Digital Twin","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/docs/digital-twin","label":"Digital Twins for Humanoid Robotics","docId":"digital-twin/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/docs/digital-twin"},{"type":"category","label":"Perception Systems","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/docs/perception","label":"Perception Systems","docId":"perception/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/docs/perception"},{"type":"category","label":"Motion Planning","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/docs/motion-planning","label":"Motion Planning","docId":"motion-planning/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/docs/motion-planning"},{"type":"category","label":"Reinforcement Learning","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/docs/reinforcement-learning","label":"Reinforcement Learning for Robotics","docId":"reinforcement-learning/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/docs/reinforcement-learning"},{"type":"category","label":"Testing & Debugging","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/docs/testing-debugging","label":"Testing & Debugging Robotics Systems","docId":"testing-debugging/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/docs/testing-debugging"},{"type":"category","label":"Isaac","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/isaac/","label":"Isaac","docId":"isaac/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/isaac/"},{"type":"category","label":"VLA (Vision-Language-Action)","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/vla/","label":"VLA (Vision-Language-Action)","docId":"vla/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/vla/"},{"type":"category","label":"Capstone Project","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/capstone/","label":"Capstone Project","docId":"capstone/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/capstone/"},{"type":"category","label":"Deployment","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/deployment/","label":"Deployment","docId":"deployment/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/deployment/"},{"type":"category","label":"Appendices","items":[{"type":"link","href":"/PhysicalAI-HumanoidRobotics/docs/appendices/","label":"Appendices","docId":"appendices/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/PhysicalAI-HumanoidRobotics/docs/appendices/"}]},"docs":{"appendices/index":{"id":"appendices/index","title":"Appendices","description":"Additional resources and reference materials for humanoid robotics development.","sidebar":"tutorialSidebar"},"capstone/index":{"id":"capstone/index","title":"Capstone Project","description":"The capstone project integrates all concepts learned throughout this guide into a comprehensive humanoid robotics application. This project demonstrates the integration of perception, planning, control, and AI systems into a functional humanoid robot.","sidebar":"tutorialSidebar"},"deployment/index":{"id":"deployment/index","title":"Deployment","description":"This section covers strategies for deploying humanoid robotics systems to production environments, including both simulation and physical hardware considerations.","sidebar":"tutorialSidebar"},"digital-twin/index":{"id":"digital-twin/index","title":"Digital Twins for Humanoid Robotics","description":"Simulation environments for testing and validating humanoid robots using Gazebo, Unity, and Isaac Sim","sidebar":"tutorialSidebar"},"foundations/embodied-intelligence":{"id":"foundations/embodied-intelligence","title":"Embodied Intelligence","description":"Intelligence shaped by geometry, environment, perception, and action","sidebar":"tutorialSidebar"},"foundations/index":{"id":"foundations/index","title":"Robotics Foundations","description":"Core concepts and mathematical foundations for humanoid robotics","sidebar":"tutorialSidebar"},"foundations/physical-ai":{"id":"foundations/physical-ai","title":"Physical AI","description":"Understanding Physical AI in humanoid robotics","sidebar":"tutorialSidebar"},"intro/index":{"id":"intro/index","title":"Introduction to Humanoid Robotics","description":"Welcome to the comprehensive guide on Physical AI and Humanoid Robotics","sidebar":"tutorialSidebar"},"isaac/index":{"id":"isaac/index","title":"Isaac","description":"Isaac is NVIDIA\'s robotics ecosystem that includes Isaac Sim, Isaac ROS, and other tools for developing and simulating robotics applications.","sidebar":"tutorialSidebar"},"motion-planning/index":{"id":"motion-planning/index","title":"Motion Planning","description":"Path planning, trajectory optimization, and locomotion for humanoid robots","sidebar":"tutorialSidebar"},"perception/index":{"id":"perception/index","title":"Perception Systems","description":"Understanding computer vision, sensor fusion, and state estimation for humanoid robots","sidebar":"tutorialSidebar"},"reinforcement-learning/index":{"id":"reinforcement-learning/index","title":"Reinforcement Learning for Robotics","description":"Applying RL and AI techniques to humanoid robot control and decision making","sidebar":"tutorialSidebar"},"ros2/humanoid-control":{"id":"ros2/humanoid-control","title":"Humanoid Control","description":"Control models for balance, locomotion, whole-body control, and manipulation","sidebar":"tutorialSidebar"},"ros2/index":{"id":"ros2/index","title":"ROS 2 - The Robotic Nervous System","description":"Learn about the Robot Operating System 2 architecture and programming for humanoid robotics","sidebar":"tutorialSidebar"},"testing-debugging/index":{"id":"testing-debugging/index","title":"Testing & Debugging Robotics Systems","description":"Comprehensive testing methodologies and debugging techniques for humanoid robots","sidebar":"tutorialSidebar"},"vla/index":{"id":"vla/index","title":"VLA (Vision-Language-Action)","description":"Vision-Language-Action (VLA) models represent a new paradigm in robotics where AI systems can understand visual input, process natural language instructions, and execute appropriate actions. These models enable robots to follow complex, natural language commands in real-world environments.","sidebar":"tutorialSidebar"}}}}')}}]);