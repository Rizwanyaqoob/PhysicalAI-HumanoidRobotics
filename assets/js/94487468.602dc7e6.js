"use strict";(globalThis.webpackChunkhumanoid_robotics_docs=globalThis.webpackChunkhumanoid_robotics_docs||[]).push([[659],{6582:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>s,contentTitle:()=>l,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"reinforcement-learning/index","title":"Reinforcement Learning for Robotics","description":"Applying RL and AI techniques to humanoid robot control and decision making","source":"@site/docs/reinforcement-learning/index.md","sourceDirName":"reinforcement-learning","slug":"/docs/reinforcement-learning","permalink":"/PhysicalAI-HumanoidRobotics/docs/docs/reinforcement-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/Rizwanyaqoob/Humanoid-Robotics-A-Practical-Introduction/tree/master/docs/reinforcement-learning/index.md","tags":[],"version":"current","frontMatter":{"title":"Reinforcement Learning for Robotics","description":"Applying RL and AI techniques to humanoid robot control and decision making","slug":"/docs/reinforcement-learning"},"sidebar":"tutorialSidebar","previous":{"title":"Motion Planning","permalink":"/PhysicalAI-HumanoidRobotics/docs/docs/motion-planning"},"next":{"title":"Reinforcement Learning for Robotics","permalink":"/PhysicalAI-HumanoidRobotics/docs/docs/reinforcement-learning"}}');var o=i(4848),r=i(8453);const a={title:"Reinforcement Learning for Robotics",description:"Applying RL and AI techniques to humanoid robot control and decision making",slug:"/docs/reinforcement-learning"},l="Reinforcement Learning for Robotics",s={},c=[{value:"Motivation",id:"motivation",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Reinforcement Learning Fundamentals",id:"reinforcement-learning-fundamentals",level:3},{value:"Deep Reinforcement Learning",id:"deep-reinforcement-learning",level:3},{value:"Applications in Humanoid Robotics",id:"applications-in-humanoid-robotics",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Q-Learning Implementation for Simple Navigation",id:"q-learning-implementation-for-simple-navigation",level:3},{value:"Actor-Critic for Continuous Control",id:"actor-critic-for-continuous-control",level:3},{value:"Code Blocks",id:"code-blocks",level:2},{value:"RL Training Loop for Humanoid Control",id:"rl-training-loop-for-humanoid-control",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Quiz",id:"quiz",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"reinforcement-learning-for-robotics",children:"Reinforcement Learning for Robotics"})}),"\n",(0,o.jsx)(e.h2,{id:"motivation",children:"Motivation"}),"\n",(0,o.jsx)(e.p,{children:"Reinforcement Learning (RL) provides a powerful framework for humanoid robots to learn complex behaviors through interaction with their environment. Unlike traditional control methods that require explicit programming, RL allows robots to discover optimal strategies through trial and error, making it particularly valuable for complex tasks where analytical solutions are difficult to obtain."}),"\n",(0,o.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,o.jsx)(e.h3,{id:"reinforcement-learning-fundamentals",children:"Reinforcement Learning Fundamentals"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs)"}),"\n",(0,o.jsx)(e.li,{children:"Reward functions and value functions"}),"\n",(0,o.jsx)(e.li,{children:"Exploration vs. exploitation trade-off"}),"\n",(0,o.jsx)(e.li,{children:"Policy gradient methods and value-based methods"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"deep-reinforcement-learning",children:"Deep Reinforcement Learning"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Deep Q-Networks (DQN) for high-dimensional state spaces"}),"\n",(0,o.jsx)(e.li,{children:"Actor-Critic methods for continuous control"}),"\n",(0,o.jsx)(e.li,{children:"Proximal Policy Optimization (PPO) for stable learning"}),"\n",(0,o.jsx)(e.li,{children:"Twin Delayed DDPG (TD3) for continuous action spaces"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"applications-in-humanoid-robotics",children:"Applications in Humanoid Robotics"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Locomotion control and gait learning"}),"\n",(0,o.jsx)(e.li,{children:"Manipulation skill acquisition"}),"\n",(0,o.jsx)(e.li,{children:"Adaptive behavior for dynamic environments"}),"\n",(0,o.jsx)(e.li,{children:"Multi-agent coordination"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,o.jsx)(e.h3,{id:"q-learning-implementation-for-simple-navigation",children:"Q-Learning Implementation for Simple Navigation"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\nimport random\n\nclass QLearningAgent:\n    def __init__(self, n_states, n_actions, learning_rate=0.1, discount=0.95, epsilon=1.0, epsilon_decay=0.995):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.lr = learning_rate\n        self.gamma = discount\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n\n        # Initialize Q-table\n        self.q_table = np.zeros((n_states, n_actions))\n\n    def choose_action(self, state):\n        """Choose action using epsilon-greedy policy"""\n        if random.uniform(0, 1) < self.epsilon:\n            # Explore: random action\n            return random.randint(0, self.n_actions - 1)\n        else:\n            # Exploit: best known action\n            return np.argmax(self.q_table[state, :])\n\n    def learn(self, state, action, reward, next_state):\n        """Update Q-table using Q-learning algorithm"""\n        best_next_action = np.argmax(self.q_table[next_state, :])\n        td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n        td_error = td_target - self.q_table[state, action]\n        self.q_table[state, action] += self.lr * td_error\n\n    def decay_epsilon(self):\n        """Decay exploration rate"""\n        self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)\n'})}),"\n",(0,o.jsx)(e.h3,{id:"actor-critic-for-continuous-control",children:"Actor-Critic for Continuous Control"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\nclass ActorCritic(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(ActorCritic, self).__init__()\n\n        # Shared feature extractor\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Actor (policy network)\n        self.actor_mean = nn.Linear(hidden_dim, action_dim)\n        self.actor_std = nn.Linear(hidden_dim, action_dim)\n\n        # Critic (value network)\n        self.critic = nn.Linear(hidden_dim, 1)\n\n    def forward(self, state):\n        features = self.feature_extractor(state)\n\n        # Actor\n        action_mean = torch.tanh(self.actor_mean(features))\n        action_std = torch.sigmoid(self.actor_std(features)) + 1e-5\n\n        # Critic\n        value = self.critic(features)\n\n        return action_mean, action_std, value\n\nclass PPOAgent:\n    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        self.actor_critic = ActorCritic(state_dim, action_dim).to(self.device)\n        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n\n    def select_action(self, state):\n        """Select action using the current policy"""\n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        action_mean, action_std, value = self.actor_critic(state)\n\n        # Sample action from normal distribution\n        dist = torch.distributions.Normal(action_mean, action_std)\n        action = dist.sample()\n\n        return action.cpu().data.numpy()[0], value.cpu().data.numpy()[0]\n'})}),"\n",(0,o.jsx)(e.h2,{id:"code-blocks",children:"Code Blocks"}),"\n",(0,o.jsx)(e.h3,{id:"rl-training-loop-for-humanoid-control",children:"RL Training Loop for Humanoid Control"}),"\n",(0,o.jsx)(e.mermaid,{value:"graph TD\n    A[Initialize Environment] --\x3e B[Get Initial State]\n    B --\x3e C[Select Action with Policy]\n    C --\x3e D[Execute Action]\n    D --\x3e E[Observe Reward and Next State]\n    E --\x3e F[Store Experience]\n    F --\x3e G{Enough Experience?}\n    G --\x3e|Yes| H[Update Policy with PPO]\n    G --\x3e|No| I[Continue Episode]\n    H --\x3e J[Log Metrics]\n    J --\x3e K{Training Complete?}\n    K --\x3e|No| B\n    K --\x3e|Yes| L[Save Model]"}),"\n",(0,o.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,o.jsx)(e.p,{children:"Common issues in RL for robotics:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Sparse reward problems"}),"\n",(0,o.jsx)(e.li,{children:"Sample efficiency and long training times"}),"\n",(0,o.jsx)(e.li,{children:"Sim-to-real transfer challenges"}),"\n",(0,o.jsx)(e.li,{children:"Safety during exploration"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"quiz",children:"Quiz"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"What is the exploration-exploitation trade-off in RL?"}),"\n",(0,o.jsx)(e.li,{children:"How does Actor-Critic differ from value-based methods?"}),"\n",(0,o.jsx)(e.li,{children:"What are the main challenges in applying RL to physical robots?"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(e.p,{children:["Continue to ",(0,o.jsx)(e.a,{href:"../testing-debugging/",children:"Testing & Debugging"})," to learn about ensuring robust and reliable robotic systems."]})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>l});var t=i(6540);const o={},r=t.createContext(o);function a(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);